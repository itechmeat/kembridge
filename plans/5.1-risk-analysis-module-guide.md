# Phase 5.1: Risk Analysis Module Implementation Guide

## üìã –û–±–∑–æ—Ä —Ñ–∞–∑—ã

**–¶–µ–ª—å:** –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ AI-–¥–≤–∏–∂–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–∏—Å–∫–æ–≤ bridge –æ–ø–µ—Ä–∞—Ü–∏–π —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤ PostgreSQL –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
**–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:** Phase 4.3 (Basic Bridge Logic), Phase 3.x (Quantum Cryptography), Phase 1.2 (Database Schema)
**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –†–∞–±–æ—Ç–∞—é—â–∏–π Python FastAPI –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å —Å ML-–∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ —Ä–∏—Å–∫–æ–≤ –∏ –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π —Å backend

## üéØ –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã

1. **ML-Powered Analysis:** –ò—Å–ø–æ–ª—å–∑—É–µ–º scikit-learn –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Ä–∏—Å–∫-–∞–Ω–∞–ª–∏–∑–∞
2. **Real-time Scoring:** –ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
3. **Historical Data Learning:** –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
4. **Quantum Integration:** –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –æ–±–º–µ–Ω –¥–∞–Ω–Ω—ã–º–∏ —Å quantum-–∑–∞—â–∏—â–µ–Ω–Ω—ã–º–∏ endpoints
5. **PostgreSQL Native:** –ü—Ä—è–º–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å transactions —Ç–∞–±–ª–∏—Ü–µ–π –∏ risk_score –ø–æ–ª—è–º–∏

## üìö –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã

### Scikit-learn –¥–ª—è Risk Analysis
- **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:** https://scikit-learn.org/stable/
- **Anomaly Detection:** IsolationForest –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
- **Classification:** LogisticRegression –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ä–∏—Å–∫–æ–≤
- **Feature Engineering:** –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ blockchain –¥–∞–Ω–Ω—ã—Ö
- **Model Persistence:** joblib –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

### FastAPI + PostgreSQL Integration
- **Async PostgreSQL:** asyncpg –¥–ª—è –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã—Ö queries
- **SQLAlchemy ORM:** –î–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å database
- **Pydantic Models:** –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥—è—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ API responses
- **Real-time Processing:** WebSocket –¥–ª—è live risk notifications

### Risk Analysis Metrics
- **Transaction Size Analysis:** –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- **Frequency Analysis:** –î–µ—Ç–µ–∫—Ü–∏—è –Ω–µ–æ–±—ã—á–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
- **Cross-chain Risk Assessment:** –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ä–∏—Å–∫–∏ –¥–ª—è ETH‚ÜîNEAR –æ–ø–µ—Ä–∞—Ü–∏–π
- **Blacklist Integration:** –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–¥—Ä–µ—Å–æ–≤ –ø—Ä–æ—Ç–∏–≤ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö blacklist'–æ–≤
- **Velocity Checks:** –ê–Ω–∞–ª–∏–∑ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –±–æ—Ç–æ–≤

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ AI Engine

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ Python –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞

```
ai-engine/
‚îú‚îÄ‚îÄ main.py                     # FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∏ endpoints
‚îú‚îÄ‚îÄ requirements.txt            # Python –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
‚îú‚îÄ‚îÄ Dockerfile                  # Docker –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚îú‚îÄ‚îÄ models/                     # ML –º–æ–¥–µ–ª–∏ –∏ persistence
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ risk_analyzer.py        # –û—Å–Ω–æ–≤–Ω–æ–π ML –¥–≤–∏–∂–æ–∫
‚îÇ   ‚îú‚îÄ‚îÄ feature_extractor.py    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö
‚îÇ   ‚îú‚îÄ‚îÄ blacklist_checker.py    # –ü—Ä–æ–≤–µ—Ä–∫–∞ blacklist –∞–¥—Ä–µ—Å–æ–≤
‚îÇ   ‚îî‚îÄ‚îÄ model_trainer.py        # –û–±—É—á–µ–Ω–∏–µ –∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
‚îú‚îÄ‚îÄ database/                   # Database –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ connection.py           # Async PostgreSQL –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
‚îÇ   ‚îú‚îÄ‚îÄ models.py               # SQLAlchemy –º–æ–¥–µ–ª–∏
‚îÇ   ‚îî‚îÄ‚îÄ queries.py              # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ queries
‚îú‚îÄ‚îÄ services/                   # Business logic —Å–µ—Ä–≤–∏—Å—ã
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ risk_service.py         # –û—Å–Ω–æ–≤–Ω–æ–π risk analysis —Å–µ—Ä–≤–∏—Å
‚îÇ   ‚îú‚îÄ‚îÄ user_profile_service.py # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ—Ñ–∏–ª–µ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
‚îÇ   ‚îî‚îÄ‚îÄ notification_service.py # Real-time —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
‚îî‚îÄ‚îÄ utils/                      # –£—Ç–∏–ª–∏—Ç—ã –∏ helpers
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ feature_utils.py        # –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è feature engineering
    ‚îú‚îÄ‚îÄ model_utils.py          # ML —É—Ç–∏–ª–∏—Ç—ã
    ‚îî‚îÄ‚îÄ cache_utils.py          # Redis –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
```

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Rust Backend

```python
# –ü—Ä–∏–º–µ—Ä –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å backend —á–µ—Ä–µ–∑ HTTP API
import httpx

class QuantumDataService:
    async def get_protected_transaction_data(
        self, 
        transaction_id: str,
        quantum_key_id: str
    ) -> dict:
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ—Ç Rust backend
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{BACKEND_URL}/api/v1/crypto/decrypt",
                json={
                    "transaction_id": transaction_id,
                    "quantum_key_id": quantum_key_id
                },
                headers={"Authorization": f"Bearer {JWT_TOKEN}"}
            )
            return response.json()
```

## üîß –ü–æ—ç—Ç–∞–ø–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

### –≠—Ç–∞–ø 1: FastAPI Setup –∏ Database Integration

**–¶–µ–ª—å:** –°–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É AI –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞ —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º –∫ PostgreSQL

```python
# main.py - –û—Å–Ω–æ–≤–Ω–æ–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
import asyncpg
import os

app = FastAPI(
    title="KEMBridge AI Risk Engine",
    description="Quantum-Secured AI Risk Analysis for Cross-Chain Bridge",
    version="0.1.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS middleware –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:4001"],  # Frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Database connection pool
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://postgres:postgres@postgres:5432/kembridge")

async def get_db():
    """Async database connection"""
    return await asyncpg.connect(DATABASE_URL)

# Basic health check
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "service": "kembridge-ai-engine",
        "version": "0.1.0",
        "ml_models": "loading"
    }
```

```python
# database/connection.py - PostgreSQL connection management
import asyncpg
import os
from contextlib import asynccontextmanager

class DatabaseManager:
    def __init__(self):
        self.database_url = os.getenv(
            "DATABASE_URL", 
            "postgresql://postgres:postgres@postgres:5432/kembridge"
        )
        self.pool = None
    
    async def create_pool(self):
        """Create connection pool"""
        self.pool = await asyncpg.create_pool(
            self.database_url,
            min_size=1,
            max_size=10,
            command_timeout=60
        )
    
    @asynccontextmanager
    async def get_connection(self):
        """Get database connection from pool"""
        async with self.pool.acquire() as connection:
            yield connection
    
    async def close_pool(self):
        """Close connection pool"""
        if self.pool:
            await self.pool.close()

# Global database manager instance
db_manager = DatabaseManager()
```

### –≠—Ç–∞–ø 2: ML Risk Analysis Engine

```python
# models/risk_analyzer.py - –û—Å–Ω–æ–≤–Ω–æ–π ML –¥–≤–∏–∂–æ–∫
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import joblib
import os
from typing import Dict, List, Tuple

class RiskAnalyzer:
    def __init__(self, model_path: str = "models/saved/"):
        self.model_path = model_path
        self.isolation_forest = None
        self.risk_classifier = None
        self.feature_scaler = None
        self.is_trained = False
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        self._load_models()
    
    def _load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            if os.path.exists(f"{self.model_path}isolation_forest.joblib"):
                self.isolation_forest = joblib.load(f"{self.model_path}isolation_forest.joblib")
            if os.path.exists(f"{self.model_path}risk_classifier.joblib"):
                self.risk_classifier = joblib.load(f"{self.model_path}risk_classifier.joblib")
            if os.path.exists(f"{self.model_path}feature_scaler.joblib"):
                self.feature_scaler = joblib.load(f"{self.model_path}feature_scaler.joblib")
            
            self.is_trained = all([
                self.isolation_forest is not None,
                self.risk_classifier is not None,
                self.feature_scaler is not None
            ])
        except Exception as e:
            print(f"Error loading models: {e}")
            self.is_trained = False
    
    def extract_features(self, transaction_data: Dict) -> np.ndarray:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è ML –∞–Ω–∞–ª–∏–∑–∞"""
        features = []
        
        # Transaction amount features
        amount = float(transaction_data.get('amount_in', 0))
        features.extend([
            amount,
            np.log1p(amount),  # Log transformation –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            min(amount / 1000, 1.0),  # Normalized amount
        ])
        
        # Chain features
        source_chain = transaction_data.get('source_chain', '')
        dest_chain = transaction_data.get('destination_chain', '')
        features.extend([
            1.0 if source_chain == 'ethereum' else 0.0,
            1.0 if dest_chain == 'near' else 0.0,
            1.0 if source_chain != dest_chain else 0.0,  # Cross-chain indicator
        ])
        
        # User history features (–±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã –∏–∑ –ë–î)
        user_history = transaction_data.get('user_history', {})
        features.extend([
            float(user_history.get('total_transactions', 0)),
            float(user_history.get('total_volume', 0)),
            float(user_history.get('avg_transaction_size', 0)),
            float(user_history.get('days_since_first_tx', 0)),
        ])
        
        # Time-based features
        hour_of_day = transaction_data.get('hour_of_day', 12)
        day_of_week = transaction_data.get('day_of_week', 1)
        features.extend([
            np.sin(2 * np.pi * hour_of_day / 24),  # –¶–∏–∫–ª–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
            np.cos(2 * np.pi * hour_of_day / 24),
            np.sin(2 * np.pi * day_of_week / 7),
            np.cos(2 * np.pi * day_of_week / 7),
        ])
        
        return np.array(features).reshape(1, -1)
    
    def analyze_risk(self, transaction_data: Dict) -> Dict:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ —Ä–∏—Å–∫–æ–≤"""
        if not self.is_trained:
            return self._fallback_analysis(transaction_data)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features = self.extract_features(transaction_data)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features_scaled = self.feature_scaler.transform(features)
        
        # Anomaly detection
        anomaly_score = self.isolation_forest.decision_function(features_scaled)[0]
        is_anomaly = self.isolation_forest.predict(features_scaled)[0] == -1
        
        # Risk classification
        risk_proba = self.risk_classifier.predict_proba(features_scaled)[0]
        risk_prediction = self.risk_classifier.predict(features_scaled)[0]
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π risk score
        combined_score = self._calculate_combined_score(
            anomaly_score, risk_proba, transaction_data
        )
        
        return {
            "risk_score": float(combined_score),
            "risk_level": self._get_risk_level(combined_score),
            "is_anomaly": bool(is_anomaly),
            "anomaly_score": float(anomaly_score),
            "ml_confidence": float(max(risk_proba)),
            "risk_factors": self._identify_risk_factors(transaction_data, features_scaled),
            "recommended_action": self._get_recommendation(combined_score),
            "approved": combined_score < 0.7
        }
    
    def _calculate_combined_score(self, anomaly_score: float, risk_proba: np.ndarray, 
                                 transaction_data: Dict) -> float:
        """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç risk score"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è anomaly score (-1 to 1) -> (0 to 1)
        normalized_anomaly = (anomaly_score + 1) / 2
        
        # ML prediction probability –¥–ª—è –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞
        ml_risk_score = risk_proba[1] if len(risk_proba) > 1 else 0.5
        
        # Rule-based risk factors
        rule_based_score = self._rule_based_analysis(transaction_data)
        
        # Weighted combination
        combined = (
            0.4 * ml_risk_score +           # ML prediction weight
            0.3 * (1 - normalized_anomaly) + # Anomaly detection weight (inverted)
            0.3 * rule_based_score           # Rule-based weight
        )
        
        return min(max(combined, 0.0), 1.0)  # Clamp to [0, 1]
    
    def _rule_based_analysis(self, transaction_data: Dict) -> float:
        """Rule-based risk analysis –∫–∞–∫ fallback"""
        risk_score = 0.1  # Base risk
        
        amount = float(transaction_data.get('amount_in', 0))
        
        # Amount-based risk
        if amount > 100:
            risk_score += 0.4
        elif amount > 10:
            risk_score += 0.2
        elif amount > 1:
            risk_score += 0.1
        
        # Cross-chain additional risk
        if (transaction_data.get('source_chain') != 
            transaction_data.get('destination_chain')):
            risk_score += 0.1
        
        # User history risk
        user_history = transaction_data.get('user_history', {})
        if user_history.get('total_transactions', 0) == 0:
            risk_score += 0.2  # New user risk
        
        return min(risk_score, 1.0)
```

### –≠—Ç–∞–ø 3: Database Integration –∏ User Profiling

```python
# services/risk_service.py - –û—Å–Ω–æ–≤–Ω–æ–π risk analysis —Å–µ—Ä–≤–∏—Å
from database.connection import db_manager
from models.risk_analyzer import RiskAnalyzer
from datetime import datetime, timedelta
import json

class RiskAnalysisService:
    def __init__(self):
        self.risk_analyzer = RiskAnalyzer()
    
    async def analyze_transaction_risk(self, user_id: str, transaction_data: Dict) -> Dict:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏"""
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_history = await self._get_user_history(user_id)
        
        # –î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∏—Å—Ç–æ—Ä–∏–µ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        enhanced_data = {
            **transaction_data,
            "user_history": user_history,
            "hour_of_day": datetime.now().hour,
            "day_of_week": datetime.now().weekday(),
        }
        
        # ML –∞–Ω–∞–ª–∏–∑
        risk_analysis = self.risk_analyzer.analyze_risk(enhanced_data)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ë–î
        await self._save_risk_analysis(user_id, transaction_data.get('transaction_id'), risk_analysis)
        
        return risk_analysis
    
    async def _get_user_history(self, user_id: str) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        async with db_manager.get_connection() as conn:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π
            query = """
            SELECT 
                COUNT(*) as total_transactions,
                COALESCE(SUM(amount_in), 0) as total_volume,
                COALESCE(AVG(amount_in), 0) as avg_transaction_size,
                COALESCE(MAX(created_at), NOW()) as last_transaction,
                COALESCE(MIN(created_at), NOW()) as first_transaction,
                COUNT(CASE WHEN risk_score > 0.5 THEN 1 END) as high_risk_count
            FROM transactions 
            WHERE user_id = $1 
                AND created_at > NOW() - INTERVAL '30 days'
                AND status IN ('completed', 'confirmed')
            """
            
            result = await conn.fetchrow(query, user_id)
            
            # –†–∞—Å—á–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            if result:
                first_tx = result['first_transaction']
                days_since_first = (datetime.now() - first_tx.replace(tzinfo=None)).days
                
                return {
                    "total_transactions": result['total_transactions'],
                    "total_volume": float(result['total_volume']),
                    "avg_transaction_size": float(result['avg_transaction_size']),
                    "days_since_first_tx": days_since_first,
                    "high_risk_ratio": (result['high_risk_count'] / max(result['total_transactions'], 1)),
                    "is_new_user": result['total_transactions'] == 0
                }
            else:
                return {
                    "total_transactions": 0,
                    "total_volume": 0.0,
                    "avg_transaction_size": 0.0,
                    "days_since_first_tx": 0,
                    "high_risk_ratio": 0.0,
                    "is_new_user": True
                }
    
    async def _save_risk_analysis(self, user_id: str, transaction_id: str, analysis: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –≤ –ë–î"""
        async with db_manager.get_connection() as conn:
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ risk_score –≤ transactions —Ç–∞–±–ª–∏—Ü–µ
            await conn.execute(
                """
                UPDATE transactions 
                SET risk_score = $1, 
                    risk_factors = $2,
                    ai_analysis_version = 'v1.0'
                WHERE id = $3
                """,
                analysis['risk_score'],
                json.dumps(analysis.get('risk_factors', {})),
                transaction_id
            )
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ audit_logs
            await conn.execute(
                """
                INSERT INTO audit_logs (
                    user_id, transaction_id, event_type, event_category,
                    event_data, severity, is_sensitive
                ) VALUES ($1, $2, $3, $4, $5, $6, $7)
                """,
                user_id,
                transaction_id,
                'ai_risk_analysis_completed',
                'security',
                json.dumps({
                    "risk_score": analysis['risk_score'],
                    "risk_level": analysis['risk_level'],
                    "ml_confidence": analysis.get('ml_confidence', 0.0),
                    "is_anomaly": analysis.get('is_anomaly', False),
                    "approved": analysis['approved']
                }),
                'warning' if analysis['risk_score'] > 0.7 else 'info',
                False
            )
```

### –≠—Ç–∞–ø 4: HTTP API Endpoints

```python
# main.py - API endpoints –¥–ª—è risk analysis
from pydantic import BaseModel
from typing import List, Optional
from services.risk_service import RiskAnalysisService

# Pydantic models –¥–ª—è API
class RiskAnalysisRequest(BaseModel):
    user_id: str
    transaction_id: Optional[str] = None
    amount_in: float
    source_chain: str
    destination_chain: str
    source_token: str
    destination_token: str

class RiskAnalysisResponse(BaseModel):
    risk_score: float
    risk_level: str
    approved: bool
    reasons: List[str]
    ml_confidence: Optional[float] = None
    is_anomaly: Optional[bool] = None
    recommended_action: str

class UserRiskProfileResponse(BaseModel):
    user_id: str
    overall_risk_level: str
    transaction_count: int
    avg_risk_score: float
    high_risk_transactions: int
    last_analysis_date: str

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
risk_service = RiskAnalysisService()

@app.post("/api/risk/analyze", response_model=RiskAnalysisResponse)
async def analyze_transaction_risk(request: RiskAnalysisRequest):
    """–ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤ –¥–ª—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏"""
    try:
        analysis = await risk_service.analyze_transaction_risk(
            request.user_id,
            request.dict()
        )
        
        return RiskAnalysisResponse(
            risk_score=analysis['risk_score'],
            risk_level=analysis['risk_level'],
            approved=analysis['approved'],
            reasons=analysis.get('risk_factors', []),
            ml_confidence=analysis.get('ml_confidence'),
            is_anomaly=analysis.get('is_anomaly'),
            recommended_action=analysis.get('recommended_action', 'proceed')
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Risk analysis failed: {str(e)}")

@app.get("/api/risk/profile/{user_id}", response_model=UserRiskProfileResponse)
async def get_user_risk_profile(user_id: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è —Ä–∏—Å–∫–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    try:
        profile = await risk_service.get_user_risk_profile(user_id)
        return UserRiskProfileResponse(**profile)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get user profile: {str(e)}")

@app.post("/api/risk/retrain")
async def retrain_models():
    """–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π (admin endpoint)"""
    try:
        result = await risk_service.retrain_models()
        return {"status": "success", "models_updated": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Model retraining failed: {str(e)}")
```

## üß™ –ü–æ—ç—Ç–∞–ø–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –≠—Ç–∞–ø 1: Database Connection Testing

```python
# tests/test_database.py
import pytest
import asyncio
from database.connection import db_manager

@pytest.mark.asyncio
async def test_database_connection():
    """–¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL"""
    await db_manager.create_pool()
    
    async with db_manager.get_connection() as conn:
        result = await conn.fetchval("SELECT 1")
        assert result == 1
    
    await db_manager.close_pool()

@pytest.mark.asyncio
async def test_user_history_query():
    """–¢–µ—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    from services.risk_service import RiskAnalysisService
    
    service = RiskAnalysisService()
    history = await service._get_user_history("test-user-id")
    
    assert isinstance(history, dict)
    assert 'total_transactions' in history
    assert 'total_volume' in history
```

### –≠—Ç–∞–ø 2: ML Model Testing

```bash
# –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ ML –º–æ–¥–µ–ª–µ–π
cd ai-engine
python -m pytest tests/test_risk_analyzer.py -v

# –†–µ–∑—É–ª—å—Ç–∞—Ç: —É—Å–ø–µ—à–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ basic risk analysis
```

### –≠—Ç–∞–ø 3: API Integration Testing

```bash
# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ API endpoints
curl -X POST "http://localhost:4003/api/risk/analyze" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "test-user",
    "amount_in": 5.0,
    "source_chain": "ethereum", 
    "destination_chain": "near",
    "source_token": "ETH",
    "destination_token": "NEAR"
  }'
```

## üìä –û—Ç–ª–æ–∂–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

### –ó–∞–¥–∞—á–∏ –∏–∑ Phase 5.1, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—å –≤ —ç—Ç–æ–π —Ñ–∞–∑–µ

| –ó–∞–¥–∞—á–∞ | –°—Ç–∞—Ç—É—Å | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |
|--------|--------|-------------|
| **3.2.7** –†–æ—Ç–∞—Ü–∏—è quantum –∫–ª—é—á–µ–π | ‚ö†Ô∏è –ü–†–ò–û–†–ò–¢–ï–¢ | –¢—Ä–µ–±—É–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∞–∫—Ç–∏–≤–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π - —Ä–µ–∞–ª–∏–∑—É–µ–º –ø–æ—Å–ª–µ –±–∞–∑–æ–≤–æ–≥–æ AI engine |
| **3.4.4** –†–æ—Ç–∞—Ü–∏—è –∫–ª—é—á–µ–π —Å HybridCrypto | ‚ö†Ô∏è –ü–†–ò–û–†–ò–¢–ï–¢ | –°–≤—è–∑–∞–Ω–æ —Å 3.2.7 |
| **4.1.9** Event listeners –¥–ª—è ETH —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π | ‚ö†Ô∏è –ü–†–ò–û–†–ò–¢–ï–¢ | –¢—Ä–µ–±—É–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å AI risk monitoring |

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥—Ä—É–≥–∏–º–∏ —Ñ–∞–∑–∞–º–∏

**Phase 5.2 Integration with Bridge Service:**
- HTTP –∫–ª–∏–µ–Ω—Ç –≤ Rust backend –¥–ª—è –≤—ã–∑–æ–≤–∞ AI —Å–µ—Ä–≤–∏—Å–∞
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –≤—ã—Å–æ–∫–æ—Ä–∏—Å–∫–æ–≤—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
- Manual review workflow –¥–ª—è –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π

**Phase 5.3 Real-time Monitoring:**
- WebSocket —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ —Ä–∏—Å–∫–∞—Ö
- Redis –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ risk scores
- Dashboard –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

## üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Rust Backend

### HTTP Client –≤ Rust –¥–ª—è AI Service

```rust
// backend/src/services/ai_risk_client.rs
use reqwest::Client;
use serde::{Deserialize, Serialize};

#[derive(Serialize)]
pub struct RiskAnalysisRequest {
    pub user_id: String,
    pub transaction_id: Option<String>,
    pub amount_in: f64,
    pub source_chain: String,
    pub destination_chain: String,
    pub source_token: String,
    pub destination_token: String,
}

#[derive(Deserialize)]
pub struct RiskAnalysisResponse {
    pub risk_score: f64,
    pub risk_level: String,
    pub approved: bool,
    pub reasons: Vec<String>,
}

pub struct AiRiskClient {
    client: Client,
    ai_engine_url: String,
}

impl AiRiskClient {
    pub fn new(ai_engine_url: String) -> Self {
        Self {
            client: Client::new(),
            ai_engine_url,
        }
    }

    pub async fn analyze_risk(&self, request: RiskAnalysisRequest) -> Result<RiskAnalysisResponse, reqwest::Error> {
        let url = format!("{}/api/risk/analyze", self.ai_engine_url);
        
        let response = self.client
            .post(&url)
            .json(&request)
            .send()
            .await?;

        response.json::<RiskAnalysisResponse>().await
    }
}
```

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ Bridge Service

```rust
// backend/crates/kembridge-bridge/src/service.rs
use crate::ai_risk_client::{AiRiskClient, RiskAnalysisRequest};

impl BridgeService {
    pub async fn init_swap_with_risk_analysis(
        &self,
        request: SwapRequest,
    ) -> Result<SwapResponse, BridgeError> {
        // 1. –ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        self.validate_swap_request(&request)?;
        
        // 2. AI Risk Analysis
        let risk_request = RiskAnalysisRequest {
            user_id: request.user_id.clone(),
            transaction_id: None,
            amount_in: request.amount_in,
            source_chain: request.source_chain.clone(),
            destination_chain: request.destination_chain.clone(),
            source_token: request.source_token.clone(),
            destination_token: request.destination_token.clone(),
        };
        
        let risk_analysis = self.ai_risk_client
            .analyze_risk(risk_request)
            .await
            .map_err(|e| BridgeError::RiskAnalysisFailed(e.to_string()))?;
        
        // 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ approved —Å—Ç–∞—Ç—É—Å–∞
        if !risk_analysis.approved {
            return Err(BridgeError::HighRiskTransaction {
                risk_score: risk_analysis.risk_score,
                reasons: risk_analysis.reasons,
            });
        }
        
        // 4. –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å bridge –æ–ø–µ—Ä–∞—Ü–∏–µ–π
        self.execute_swap_with_risk_score(request, risk_analysis.risk_score).await
    }
}
```

## üìà –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

### –ü–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ Phase 5.1

- ‚úÖ **Python FastAPI –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å** —Å ML-–∞–Ω–∞–ª–∏–∑–æ–º —Ä–∏—Å–∫–æ–≤
- ‚úÖ **PostgreSQL –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** —Å transactions —Ç–∞–±–ª–∏—Ü–µ–π 
- ‚úÖ **Scikit-learn –º–æ–¥–µ–ª–∏** –¥–ª—è anomaly detection –∏ classification
- ‚úÖ **HTTP API endpoints** –¥–ª—è risk analysis
- ‚úÖ **User profiling system** –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
- ‚úÖ **Blacklist checking** –∏ rule-based analysis
- ‚úÖ **Real-time risk scoring** –¥–ª—è –∫–∞–∂–¥–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏

### –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ Phase 5.2

```mermaid
graph TB
    subgraph "Phase 5.1: AI Risk Engine - –ì–û–¢–û–í–û"
        A[FastAPI Microservice] --> B[ML Risk Models]
        A --> C[PostgreSQL Integration]
        A --> D[HTTP API]
        E[User Profiling] --> A
        F[Blacklist Checker] --> A
    end
    
    subgraph "Phase 5.2: Bridge Integration"
        G[Rust HTTP Client] --> A
        H[Auto Risk Blocking] --> G
        I[Manual Review Workflow] --> H
    end
    
    style A fill:#90EE90
    style B fill:#90EE90
    style C fill:#90EE90
    style D fill:#90EE90
    style E fill:#90EE90
    style F fill:#90EE90
```

## üìñ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

### Machine Learning Documentation
- **Scikit-learn User Guide:** https://scikit-learn.org/stable/user_guide.html
- **Anomaly Detection:** https://scikit-learn.org/stable/modules/outlier_detection.html
- **Model Persistence:** https://scikit-learn.org/stable/model_persistence.html

### FastAPI Documentation
- **FastAPI Tutorial:** https://fastapi.tiangolo.com/tutorial/
- **Async Dependencies:** https://fastapi.tiangolo.com/async/
- **Background Tasks:** https://fastapi.tiangolo.com/tutorial/background-tasks/

### PostgreSQL Async Integration
- **asyncpg Documentation:** https://magicstack.github.io/asyncpg/current/
- **SQLAlchemy Async:** https://docs.sqlalchemy.org/en/20/orm/extensions/asyncio.html

---

**Phase 5.1 Status:** –ì–æ—Ç–æ–≤ –∫ –ø–æ—ç—Ç–∞–ø–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π ML-–∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, PostgreSQL database –∏ HTTP API –¥–ª—è seamless –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Rust backend. AI Risk Engine –±—É–¥–µ—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—É—é bridge —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å quantum-–∑–∞—â–∏—â–µ–Ω–Ω—ã–º –æ–±–º–µ–Ω–æ–º –¥–∞–Ω–Ω—ã–º–∏.